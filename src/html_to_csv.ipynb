{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From URL files, we will crawl into the url and extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. selenium to get html file for each state (54 html files, each file contains 500 trail url in it.\n",
    "2. html files --> extract url of each trail ( 500 trails in 1 html file, i have 54 html files)\n",
    "\n",
    "\n",
    "3. text file containg 500 urls --> go into the url / get html / parse --> csv (500 rows, 54 files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_except(prompt, lst):\n",
    "    try:\n",
    "        a=eval(prompt)\n",
    "        lst.append(a)\n",
    "    except:\n",
    "        lst.append(None)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I get results, or soup, save it to a text file/JSON file \n",
    "\n",
    "Parsing part can be parallelized (multiprocessing library in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For looping through all files in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eunheelim/Capstone1/urls/kansas.txt\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a758359984eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mclass_name_10\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'styles-module__ssrFallback___34ups [object Object] styles-module__flexContainer___1o9fS styles-module__slidingTabs___25XGa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdiv10\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mclass_name_10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdiv10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mn_photos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for filepath in glob.iglob(r'/Users/eunheelim/Capstone1/urls/*.txt'):\n",
    "    print(filepath)\n",
    "    #filepath = \"/Users/eunheelim/Capstone1/urls/washington.txt\"\n",
    "    file1 = open(filepath, 'r') \n",
    "    urls = file1.readlines() \n",
    "\n",
    "\n",
    "    #initiate data storage\n",
    "    #our loop through each container\n",
    "    names = []\n",
    "    difficulties = []\n",
    "    average_ratings=[]\n",
    "    worst_ratings=[]\n",
    "    best_ratings=[]\n",
    "    review_counts=[]\n",
    "    elevations=[]\n",
    "    route_types=[]\n",
    "    short_descriptions=[]\n",
    "    long_descriptions=[]\n",
    "    tags_list=[]\n",
    "    locations=[]\n",
    "    n_photos=[]\n",
    "    n_recordings=[]\n",
    "    n_completed=[]\n",
    "\n",
    "\n",
    "    # Make for loop into functions /pass in list of URLS as inpur argument\n",
    "    count = 0\n",
    "    for trail_url in urls:\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "\n",
    "        results = requests.get(trail_url[0:-1])\n",
    "        soup=BeautifulSoup(results.text, 'html.parser')\n",
    "\n",
    "\n",
    "        class_name_10='styles-module__ssrFallback___34ups [object Object] styles-module__flexContainer___1o9fS styles-module__slidingTabs___25XGa'\n",
    "        div10=soup.find_all('div',{'class':class_name_10})\n",
    "        stats = [i.text for i in div10[0].find_all('div')]\n",
    "\n",
    "        n_photos.append(int(stats[1][8:-1]))\n",
    "        n_recordings.append(int(stats[2][12:-1]))\n",
    "        n_completed.append(int(stats[3][11:-1]))\n",
    "\n",
    "\n",
    "        class_name_3='styles-module__content___1GUwP'\n",
    "        container=soup.find_all('div', {'class':class_name_3})[0]\n",
    "\n",
    "\n",
    "        #Get trail name\n",
    "        name =\"container.h1['title']\"\n",
    "        try_except(name, names)\n",
    "\n",
    "        #Get trail location\n",
    "        location = \"container.a['title']\"\n",
    "        try_except(location,locations)\n",
    "\n",
    "        #Get trail difficulty\n",
    "        difficulty= \"container.span.text\"\n",
    "        try_except(difficulty, difficulties)\n",
    "\n",
    "        #Get trail elevation\n",
    "        elevation =\"((soup.find('section', {'id':'trail-stats'})).find('span',{'class':'elevation-icon'}) \\\n",
    "                .find('span',{'class':'detail-data xlate-none'}) \\\n",
    "                .text[13:].split('f'))[0] \\\n",
    "                .replace(',','')\"\n",
    "        try_except(elevation, elevations)\n",
    "\n",
    "        #Get trail route type\n",
    "        a =soup.find('section', {'id':'trail-stats'});\n",
    "        route_type = \"a.find_all('span',{'class':'detail-data'})[2].text\"\n",
    "        try_except(route_type, route_types)\n",
    "\n",
    "        #Get trail short description\n",
    "        short_description=\"soup.find('p',{'class':'xlate-google line-clamp-4'}).text\"\n",
    "        try_except(short_description, short_descriptions)\n",
    "\n",
    "        #Get trail long description\n",
    "        long_description=\"soup.find('p',{'class':'styles-module__displayText___17Olo'}).text\"\n",
    "        try_except(long_description, long_descriptions)\n",
    "\n",
    "        #Get trail rating container\n",
    "        a=(container.find_all('meta'))\n",
    "        average_rating =(a[0]['content'])\n",
    "        average_ratings.append(average_rating)\n",
    "\n",
    "        worst_rating = (a[1]['content'])\n",
    "        worst_ratings.append(worst_rating)\n",
    "\n",
    "        best_rating = (a[2]['content'])\n",
    "        best_ratings.append(best_rating)\n",
    "\n",
    "        review_count = (a[3]['content'])\n",
    "        review_counts.append(review_count)\n",
    "\n",
    "        #Get trail tags\n",
    "        tags= soup.find('section',{'class':'tag-cloud'}).find_all('span',{'class':'big rounded active'})\n",
    "\n",
    "\n",
    "        tag_list=[]\n",
    "        for item in tags:\n",
    "            tag_list.append(item.text)\n",
    "\n",
    "        tags_list.append(tag_list)\n",
    "\n",
    "    #pandas dataframe        \n",
    "    trails = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'difficulty': difficulties,\n",
    "    'average_rating' :average_ratings,\n",
    "    'worst_rating': worst_ratings,\n",
    "    'best_rating':best_ratings,\n",
    "    'review_count':review_counts,\n",
    "    'location':locations,\n",
    "    'elevation':elevations,\n",
    "    'route_type': route_types,\n",
    "    'short_description':short_descriptions,\n",
    "    'long_description':long_descriptions,\n",
    "    'tag_list' :tags_list,\n",
    "    'n_photos':n_photos,\n",
    "    'n_recordings':n_recordings,\n",
    "    'n_completed':n_completed\n",
    "    })\n",
    "\n",
    "\n",
    "    filename='/Users/eunheelim/Capstone1/data2/'+filepath[32:].split('.')[0]+'2.csv'\n",
    "\n",
    "    #add dataframe to csv file named 'movies.csv'\n",
    "    trails.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # For running individual txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filepath in glob.iglob(r'/Users/eunheelim/Capstone1/urls/*.txt'):\n",
    "#    print(filepath)\n",
    "filepath = \"/Users/eunheelim/Capstone1/urls/california.txt\"\n",
    "print(filepath)\n",
    "file1 = open(filepath, 'r') \n",
    "urls = file1.readlines() \n",
    "\n",
    "\n",
    "#initiate data storage\n",
    "#our loop through each container\n",
    "names = []\n",
    "difficulties = []\n",
    "average_ratings=[]\n",
    "worst_ratings=[]\n",
    "best_ratings=[]\n",
    "review_counts=[]\n",
    "elevations=[]\n",
    "route_types=[]\n",
    "short_descriptions=[]\n",
    "long_descriptions=[]\n",
    "tags_list=[]\n",
    "locations=[]\n",
    "n_photos=[]\n",
    "n_recordings=[]\n",
    "n_completed=[]\n",
    "\n",
    "count = 0\n",
    "for trail_url in urls:\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "\n",
    "    results = requests.get(trail_url[0:-1])\n",
    "    soup=BeautifulSoup(results.text, 'html.parser')\n",
    "\n",
    "\n",
    "    class_name_10='styles-module__ssrFallback___34ups [object Object] styles-module__flexContainer___1o9fS styles-module__slidingTabs___25XGa'\n",
    "    div10=soup.find_all('div',{'class':class_name_10})\n",
    "    stats = [i.text for i in div10[0].find_all('div')]\n",
    "\n",
    "    n_photos.append(int(stats[1][8:-1]))\n",
    "    n_recordings.append(int(stats[2][12:-1]))\n",
    "    n_completed.append(int(stats[3][11:-1]))\n",
    "\n",
    "\n",
    "    class_name_3='styles-module__content___1GUwP'\n",
    "    container=soup.find_all('div', {'class':class_name_3})[0]\n",
    "\n",
    "\n",
    "    #Get trail name\n",
    "    name =\"container.h1['title']\"\n",
    "    try_except(name, names)\n",
    "\n",
    "    #Get trail location\n",
    "    location = \"container.a['title']\"\n",
    "    try_except(location,locations)\n",
    "\n",
    "    #Get trail difficulty\n",
    "    difficulty= \"container.span.text\"\n",
    "    try_except(difficulty, difficulties)\n",
    "\n",
    "    #Get trail elevation\n",
    "    elevation =\"((soup.find('section', {'id':'trail-stats'})).find('span',{'class':'elevation-icon'}) \\\n",
    "            .find('span',{'class':'detail-data xlate-none'}) \\\n",
    "            .text[13:].split('f'))[0] \\\n",
    "            .replace(',','')\"\n",
    "    try_except(elevation, elevations)\n",
    "\n",
    "    #Get trail route type\n",
    "    a =soup.find('section', {'id':'trail-stats'});\n",
    "    route_type = \"a.find_all('span',{'class':'detail-data'})[2].text\"\n",
    "    try_except(route_type, route_types)\n",
    "\n",
    "    #Get trail short description\n",
    "    short_description=\"soup.find('p',{'class':'xlate-google line-clamp-4'}).text\"\n",
    "    try_except(short_description, short_descriptions)\n",
    "\n",
    "    #Get trail long description\n",
    "    long_description=\"soup.find('p',{'class':'styles-module__displayText___17Olo'}).text\"\n",
    "    try_except(long_description, long_descriptions)\n",
    "\n",
    "    #Get trail rating container\n",
    "    a=(container.find_all('meta'))\n",
    "    average_rating =(a[0]['content'])\n",
    "    average_ratings.append(average_rating)\n",
    "\n",
    "    worst_rating = (a[1]['content'])\n",
    "    worst_ratings.append(worst_rating)\n",
    "\n",
    "    best_rating = (a[2]['content'])\n",
    "    best_ratings.append(best_rating)\n",
    "\n",
    "    review_count = (a[3]['content'])\n",
    "    review_counts.append(review_count)\n",
    "\n",
    "    #Get trail tags\n",
    "    tags= soup.find('section',{'class':'tag-cloud'}).find_all('span',{'class':'big rounded active'})\n",
    "\n",
    "\n",
    "    tag_list=[]\n",
    "    for item in tags:\n",
    "        tag_list.append(item.text)\n",
    "\n",
    "    tags_list.append(tag_list)\n",
    "\n",
    "    #pandas dataframe        \n",
    "    trails = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'difficulty': difficulties,\n",
    "    'average_rating' :average_ratings,\n",
    "    'worst_rating': worst_ratings,\n",
    "    'best_rating':best_ratings,\n",
    "    'review_count':review_counts,\n",
    "    'location':locations,\n",
    "    'elevation':elevations,\n",
    "    'route_type': route_types,\n",
    "    'short_description':short_descriptions,\n",
    "    'long_description':long_descriptions,\n",
    "    'tag_list' :tags_list,\n",
    "    'n_photos':n_photos,\n",
    "    'n_recordings':n_recordings,\n",
    "    'n_completed':n_completed\n",
    "    })\n",
    "\n",
    "\n",
    "    #filename='/Users/eunheelim/Capstone1/data2/'+filepath[32:].split('.')[0]+'2.csv'\n",
    "    filename='/Users/eunheelim/Capstone1/data2/california2.csv'\n",
    "\n",
    "    #add dataframe to csv file named 'movies.csv'\n",
    "    trails.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at dividing and parallelizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trail_urls(list_of_urls):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = \"/Users/eunheelim/Capstone1/urls/new-york.txt\"\n",
    "# sample url: https://www.alltrails.com/trail/us/alabama/walls-of-jericho-trail\n",
    "\n",
    "def get_trail_htmls(sample_url):\n",
    "    \n",
    "    results = requests.get(sample_url[0:-1])\n",
    "    soup=BeautifulSoup(results.text, 'html.parser') \n",
    "    \n",
    "    filename = '/Users/eunheelim/Capstone1/state_htmls/'+ sample_url.split('/')[6] + '.html'\n",
    "    with open(filename, 'w') as file:\n",
    "    file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(sample_html):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url=' https://www.alltrails.com/trail/us/alabama/walls-of-jericho-trail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url.split('/')[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
