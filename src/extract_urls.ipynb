{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Getting URLS\n",
    "\n",
    "- From the 54 HTML files, I will use beautifulSoup to create 54 text files including URL for individual trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_except(prompt, lst):\n",
    "    try:\n",
    "        a=eval(prompt)\n",
    "        lst.append(a)\n",
    "    except:\n",
    "        lst.append(None)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name=\"styles-module__containerDescriptive___3aZqQ styles-module__trailCard___2oHiP\"\n",
    "class_name_2='xlate-none styles-module__link___12BPT'\n",
    "class_name_4='styles-module__content___1eARw styles-module__content___3dWXB'\n",
    "class_name_6='styles-module__info___1Mbn6'\n",
    "class_name_8=\"xlate-none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eunheelim/Capstone1/test_html/ohio.html\n",
      "/Users/eunheelim/Capstone1/test_html/new-york.html\n",
      "/Users/eunheelim/Capstone1/test_html/oregon.html\n",
      "/Users/eunheelim/Capstone1/test_html/louisiana.html\n",
      "/Users/eunheelim/Capstone1/test_html/illinois.html\n",
      "/Users/eunheelim/Capstone1/test_html/north-carolina.html\n",
      "/Users/eunheelim/Capstone1/test_html/mississippi.html\n",
      "/Users/eunheelim/Capstone1/test_html/nevada.html\n",
      "/Users/eunheelim/Capstone1/test_html/tennessee.html\n",
      "/Users/eunheelim/Capstone1/test_html/new-jersey.html\n",
      "/Users/eunheelim/Capstone1/test_html/minnesota.html\n",
      "/Users/eunheelim/Capstone1/test_html/montana.html\n",
      "/Users/eunheelim/Capstone1/test_html/south-carolina.html\n",
      "/Users/eunheelim/Capstone1/test_html/new-hampshire.html\n",
      "/Users/eunheelim/Capstone1/test_html/utah.html\n",
      "/Users/eunheelim/Capstone1/test_html/rhode-island.html\n",
      "/Users/eunheelim/Capstone1/test_html/florida.html\n",
      "/Users/eunheelim/Capstone1/test_html/vermont.html\n",
      "/Users/eunheelim/Capstone1/test_html/oklahoma.html\n",
      "/Users/eunheelim/Capstone1/test_html/delaware.html\n",
      "/Users/eunheelim/Capstone1/test_html/west-virginia.html\n",
      "/Users/eunheelim/Capstone1/test_html/indiana.html\n",
      "/Users/eunheelim/Capstone1/test_html/north-dakota.html\n",
      "/Users/eunheelim/Capstone1/test_html/michigan.html\n",
      "/Users/eunheelim/Capstone1/test_html/kentucky.html\n",
      "/Users/eunheelim/Capstone1/test_html/new-mexico.html\n",
      "/Users/eunheelim/Capstone1/test_html/washington.html\n",
      "/Users/eunheelim/Capstone1/test_html/south-dakota.html\n",
      "/Users/eunheelim/Capstone1/test_html/connecticut.html\n",
      "/Users/eunheelim/Capstone1/test_html/pennsylvania.html\n",
      "/Users/eunheelim/Capstone1/test_html/maryland.html\n",
      "/Users/eunheelim/Capstone1/test_html/california.html\n",
      "/Users/eunheelim/Capstone1/test_html/virginia.html\n",
      "/Users/eunheelim/Capstone1/test_html/hawaii.html\n",
      "/Users/eunheelim/Capstone1/test_html/alabama.html\n",
      "/Users/eunheelim/Capstone1/test_html/colorado.html\n",
      "/Users/eunheelim/Capstone1/test_html/arkansas.html\n",
      "/Users/eunheelim/Capstone1/test_html/missouri.html\n",
      "/Users/eunheelim/Capstone1/test_html/iowa.html\n",
      "/Users/eunheelim/Capstone1/test_html/maine.html\n",
      "/Users/eunheelim/Capstone1/test_html/kansas.html\n",
      "/Users/eunheelim/Capstone1/test_html/alaska.html\n",
      "/Users/eunheelim/Capstone1/test_html/wyoming.html\n",
      "/Users/eunheelim/Capstone1/test_html/arizona.html\n",
      "/Users/eunheelim/Capstone1/test_html/wisconsin.html\n",
      "/Users/eunheelim/Capstone1/test_html/georgia.html\n",
      "/Users/eunheelim/Capstone1/test_html/texas.html\n",
      "/Users/eunheelim/Capstone1/test_html/idaho.html\n",
      "/Users/eunheelim/Capstone1/test_html/massachusetts.html\n",
      "/Users/eunheelim/Capstone1/test_html/nebraska.html\n"
     ]
    }
   ],
   "source": [
    "for filepath in glob.iglob(r'/Users/eunheelim/Capstone1/test_html/*.html'):\n",
    "    \n",
    "    print(filepath)\n",
    "    state=(filepath[37:].split('.')[0])\n",
    "    #print(state)\n",
    "    \n",
    "    soup = BeautifulSoup(open(filepath), \"html.parser\")\n",
    "    \n",
    "    each_urls=[]\n",
    "    durations=[]\n",
    "    distances=[]\n",
    "    \n",
    "    div = soup.find_all('div', {'class':class_name})\n",
    "    div2=soup.find_all('div', {'class':class_name_2})\n",
    "    div4=soup.find_all('div', {'class':class_name_4})\n",
    "\n",
    "    \n",
    "    for container in div:\n",
    "        \n",
    "        #Get list of url to .txt file\n",
    "        each_url = 'https://www.alltrails.com' + (container.a['href'])\n",
    "        each_url=each_url.replace('?ref=result-card','')\n",
    "        each_urls.append(each_url)\n",
    "        \n",
    "        #Save list of url to .txt file\n",
    "        filename = '/Users/eunheelim/Capstone1/urls/'+ state+'.txt'\n",
    "        np.savetxt(filename, each_urls, fmt=\"%s\", delimiter=',')\n",
    "        \n",
    "        div6=container.find_all('div', {'class':class_name_6})\n",
    "        \n",
    "        #Get list of durations for each of 500 trail\n",
    "        duration = \"((div6[1].find_all('span', {'class':class_name_8}))[1].text)[5:]\"\n",
    "        try_except(duration, durations)\n",
    "        \n",
    "        #Get list of distances for each of 500 trail\n",
    "        distance=\"(((div6[1].find_all('span', {'class':class_name_8}))[0].text))[8:]\"\n",
    "        try_except(distance, distances)\n",
    "\n",
    "        filename = '/Users/eunheelim/Capstone1/data/'+ state+'.csv'\n",
    "        np.savetxt(filename, np.c_[ distances, durations], fmt=\"%s\", delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_states=['hawaii', 'washington', 'arizona','oregon','missouri','georgia','maine','maryland','mississippi','iowa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eunheelim/Capstone1/test_html/hawaii.html\n",
      "/Users/eunheelim/Capstone1/test_html/washington.html\n",
      "/Users/eunheelim/Capstone1/test_html/arizona.html\n",
      "/Users/eunheelim/Capstone1/test_html/oregon.html\n",
      "/Users/eunheelim/Capstone1/test_html/missouri.html\n",
      "/Users/eunheelim/Capstone1/test_html/georgia.html\n",
      "/Users/eunheelim/Capstone1/test_html/maine.html\n",
      "/Users/eunheelim/Capstone1/test_html/maryland.html\n",
      "/Users/eunheelim/Capstone1/test_html/mississippi.html\n",
      "/Users/eunheelim/Capstone1/test_html/iowa.html\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_states)):\n",
    "    filepath = '/Users/eunheelim/Capstone1/test_html/'+new_states[i]+'.html'\n",
    "    #for filepath in glob.iglob(r'/Users/eunheelim/Capstone1/test_html/*.html'):\n",
    "    print(filepath)\n",
    "    state=(filepath[37:].split('.')[0])\n",
    "    #print(state)\n",
    "\n",
    "    soup = BeautifulSoup(open(filepath), \"html.parser\")\n",
    "\n",
    "    each_urls=[]\n",
    "    durations=[]\n",
    "    distances=[]\n",
    "\n",
    "    div = soup.find_all('div', {'class':class_name})\n",
    "    div2=soup.find_all('div', {'class':class_name_2})\n",
    "    div4=soup.find_all('div', {'class':class_name_4})\n",
    "\n",
    "\n",
    "    for container in div:\n",
    "\n",
    "        #Get list of url to .txt file\n",
    "        each_url = 'https://www.alltrails.com' + (container.a['href'])\n",
    "        each_url=each_url.replace('?ref=result-card','')\n",
    "        each_urls.append(each_url)\n",
    "\n",
    "        #Save list of url to .txt file\n",
    "        filename = '/Users/eunheelim/Capstone1/urls/'+ state+'.txt'\n",
    "        np.savetxt(filename, each_urls, fmt=\"%s\", delimiter=',')\n",
    "\n",
    "        div6=container.find_all('div', {'class':class_name_6})\n",
    "\n",
    "        #Get list of durations for each of 500 trail\n",
    "        duration = \"((div6[1].find_all('span', {'class':class_name_8}))[1].text)[5:]\"\n",
    "        try_except(duration, durations)\n",
    "\n",
    "        #Get list of distances for each of 500 trail\n",
    "        distance=\"(((div6[1].find_all('span', {'class':class_name_8}))[0].text))[8:]\"\n",
    "        try_except(distance, distances)\n",
    "\n",
    "        filename = '/Users/eunheelim/Capstone1/data/'+ state+'2.csv'\n",
    "        np.savetxt(filename, np.c_[ distances, durations], fmt=\"%s\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
