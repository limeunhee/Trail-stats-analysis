{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From URL files, we will crawl into the url and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_except(prompt, lst):\n",
    "    try:\n",
    "        a=eval(prompt)\n",
    "        print(a)\n",
    "        lst.append(a)\n",
    "    except:\n",
    "        print('No')\n",
    "        lst.append(None)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('1+1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abcd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ff9dbe29f57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abcd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'abcd' is not defined"
     ]
    }
   ],
   "source": [
    "eval('abcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping process flow\n",
    "\n",
    "1. Get & save 54 html files for each state (each contains url for 500 hikes in one state)\n",
    "2. Generate list of urls for each hike ( total 54 text files, each containing 500 urls)\n",
    "3. From the text file --> get 1 url --> soup --> parse -> save to csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I get results, or soup, save it to a text file/JSON file \n",
    "\n",
    "Parsing part can be parallelized (multiprocessing library in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For looping through all files in the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For running individual txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at dividing and parallelizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trail_urls(list_of_urls):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trail_htmls(sample_url):\n",
    "    results = requests.get(sample_url)\n",
    "    soup=BeautifulSoup(results.text, 'html.parser') \n",
    "    \n",
    "    filename = '/Users/eunheelim/Capstone1/pl_test/'+ sample_url.split('/')[6] + '.html'\n",
    "    print(filename)\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through txt file to get all the htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eunheelim/Capstone1/pl_test/walls-of-jericho-trail.html\n"
     ]
    }
   ],
   "source": [
    "sample_url= 'https://www.alltrails.com/trail/us/alabama/walls-of-jericho-trail'\n",
    "get_trail_htmls(sample_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walls of Jericho Trail\n",
      "Raven Rocks via Appalachian Trail\n",
      "1: Walls of Jericho Trail\n",
      "2: Walls of Jericho Trail\n",
      "Bluemont, Virginia\n",
      "hard\n",
      "1617 \n",
      "No\n",
      "Raven Rocks via Appalachian Trail is a 5.3 mile heavily trafficked out and back trail located near Bluemont, Virginia that features a river and is rated as difficult. The trail offers a number of activity options and is accessible year-round. Dogs are also able to use this trail.\n",
      "This is a moderate to hard 4-view trail with the final view point being the most magnificent. This out-and-back hike -- which some locals call \"the roller-coaster\" -- winds up and down several ridges, some of which are fairly steep.\n",
      " \n",
      "Please note that parking is scarce.\n"
     ]
    }
   ],
   "source": [
    "soup_file = '/Users/eunheelim/Capstone1/pl_test/walls-of-jericho-trail.html'\n",
    "parse_data(soup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA = []\n",
    "\n",
    "for html_file in file_list:\n",
    "    a = parse_data(url)   --> ['name','distance', etc]\n",
    "    CA.append(a)\n",
    "    \n",
    "    return dataframe or file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(soup_file):\n",
    "    \n",
    "    #initiate data storage\n",
    "    #our loop through each container\n",
    "    names = []\n",
    "    difficulties = []\n",
    "    average_ratings=[]\n",
    "    worst_ratings=[]\n",
    "    best_ratings=[]\n",
    "    review_counts=[]\n",
    "    elevations=[]\n",
    "    route_types=[]\n",
    "    short_descriptions=[]\n",
    "    long_descriptions=[]\n",
    "    tags_list=[]\n",
    "    locations=[]\n",
    "    n_photos=[]\n",
    "    n_recordings=[]\n",
    "    n_completed=[]\n",
    "    \n",
    "    soup = BeautifulSoup(open(soup_file), \"html.parser\")\n",
    "    \n",
    "    class_name_10='styles-module__ssrFallback___34ups [object Object] styles-module__flexContainer___1o9fS styles-module__slidingTabs___25XGa'\n",
    "    div10=soup.find_all('div',{'class':class_name_10})\n",
    "    stats = [i.text for i in div10[0].find_all('div')]\n",
    "\n",
    "    n_photos.append(int(stats[1][8:-1]))\n",
    "    n_recordings.append(int(stats[2][12:-1]))\n",
    "    n_completed.append(int(stats[3][11:-1]))\n",
    "\n",
    "\n",
    "    class_name_3='styles-module__content___1GUwP'\n",
    "    container=soup.find_all('div', {'class':class_name_3})[0]\n",
    "    print(container.h1['title'])\n",
    "\n",
    "     \n",
    "    #Get trail name           \n",
    "    name1 =\"container.h1['title']\"\n",
    "    name =container.h1['title']\n",
    "    try_except(name1, names)\n",
    "    print('1:', eval(name1))\n",
    "    print('2:',name)\n",
    "\n",
    "\n",
    "\n",
    "    #Get trail location\n",
    "    location = \"container.a['title']\"\n",
    "    try_except(location,locations)\n",
    "\n",
    "    #Get trail difficulty\n",
    "    difficulty= \"container.span.text\"\n",
    "    try_except(difficulty, difficulties)\n",
    "\n",
    "    #Get trail elevation\n",
    "    elevation =\"((soup.find('section', {'id':'trail-stats'})).find('span',{'class':'elevation-icon'}) \\\n",
    "            .find('span',{'class':'detail-data xlate-none'}) \\\n",
    "            .text[13:].split('f'))[0] \\\n",
    "            .replace(',','')\"\n",
    "    try_except(elevation, elevations)\n",
    "\n",
    "    #Get trail route type\n",
    "    a =soup.find('section', {'id':'trail-stats'});\n",
    "    route_type = \"a.find_all('span',{'class':'detail-data'})[2].text\"\n",
    "    try_except(route_type, route_types)\n",
    "\n",
    "    #Get trail short description\n",
    "    short_description=\"soup.find('p',{'class':'xlate-google line-clamp-4'}).text\"\n",
    "    try_except(short_description, short_descriptions)\n",
    "\n",
    "    #Get trail long description\n",
    "    long_description=\"soup.find('p',{'class':'styles-module__displayText___17Olo'}).text\"\n",
    "    try_except(long_description, long_descriptions)\n",
    "\n",
    "    #Get trail rating container\n",
    "    a=(container.find_all('meta'))\n",
    "    average_rating =(a[0]['content'])\n",
    "    average_ratings.append(average_rating)\n",
    "\n",
    "    worst_rating = (a[1]['content'])\n",
    "    worst_ratings.append(worst_rating)\n",
    "\n",
    "    best_rating = (a[2]['content'])\n",
    "    best_ratings.append(best_rating)\n",
    "\n",
    "    review_count = (a[3]['content'])\n",
    "    review_counts.append(review_count)\n",
    "\n",
    "    #Get trail tags\n",
    "    tags= soup.find('section',{'class':'tag-cloud'}).find_all('span',{'class':'big rounded active'})\n",
    "\n",
    "\n",
    "    tag_list=[]\n",
    "    for item in tags:\n",
    "        tag_list.append(item.text)\n",
    "\n",
    "    tags_list.append(tag_list)\n",
    "    \n",
    "    #pandas dataframe        \n",
    "    trails = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'difficulty': difficulties,\n",
    "    'average_rating' :average_ratings,\n",
    "    'worst_rating': worst_ratings,\n",
    "    'best_rating':best_ratings,\n",
    "    'review_count':review_counts,\n",
    "    'location':locations,\n",
    "    'elevation':elevations,\n",
    "    'route_type': route_types,\n",
    "    'short_description':short_descriptions,\n",
    "    'long_description':long_descriptions,\n",
    "    'tag_list' :tags_list,\n",
    "    'n_photos':n_photos,\n",
    "    'n_recordings':n_recordings,\n",
    "    'n_completed':n_completed\n",
    "    })\n",
    "    \n",
    "    filename='/Users/eunheelim/Capstone1/pl_test/'+soup_file.split('/')[5]+'2.csv'\n",
    "\n",
    "    #add dataframe to csv file named 'movies.csv'\n",
    "    trails.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url=' https://www.alltrails.com/trail/us/alabama/walls-of-jericho-trail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url.split('/')[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
